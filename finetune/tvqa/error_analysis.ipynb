{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf71fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Finetunes TVQA.\n",
    "\n",
    "I used a v3-32 for this (it's more expensive than VCR due to the use of video + sound data)\n",
    "\"\"\"\n",
    "\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "sys.path.append('../../')\n",
    "sys.path.append('/home/sheryl/merlot_reserve')\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from pretrain.dataloader import input_fn_builder, MASK, encoder, AUDIOSPAN\n",
    "from finetune.common_dataloader import finetune_input_fn_builder, finetune_val_input_fn_builder\n",
    "from mreserve.modeling import MerlotReserve\n",
    "\n",
    "from flax.training import train_state\n",
    "from flax import jax_utils\n",
    "import flax.linen as nn\n",
    "from finetune.optimization import construct_finetuning_train_state, finetune_train_step\n",
    "from mreserve.checkpoint import save_checkpoint, load_checkpoint, bf16_to_f32, f32_to_bf16\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from flax.core.frozen_dict import freeze\n",
    "from copy import deepcopy\n",
    "import clu.parameter_overview\n",
    "import functools\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "from mreserve.modeling import PretrainedMerlotReserve\n",
    "\n",
    "load_dotenv('../../.env')\n",
    "\n",
    "jax.config.update('jax_log_compiles', True)\n",
    "is_on_gpu = any([x.platform == 'gpu' for x in jax.local_devices()])\n",
    "print('JAX process: {} / {}. Local devices {}. Using {}'.format(jax.process_index(), jax.process_count(), jax.local_devices(), 'GPU' if is_on_gpu else 'TPU'), flush=True)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Train model!')\n",
    "\n",
    "parser.add_argument(\n",
    "    '-pretrain_config_file',\n",
    "    help='Where the config.yaml is located',\n",
    "    type=str,\n",
    "    default=\"../../pretrain/configs/base.yaml\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-ckpt',\n",
    "    help='checkpoint to use',\n",
    "    type=str,\n",
    "    default=\"../../../base_resadapt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--lr',\n",
    "    help='lr',\n",
    "    type=float,\n",
    "    default=5e-6,\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-ne',\n",
    "    help='ne',\n",
    "    type=int,\n",
    "    default=20,\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-output_grid_h',\n",
    "    help='output_grid_h',\n",
    "    type=int,\n",
    "    default=18,\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-output_grid_w',\n",
    "    help='output_grid_w',\n",
    "    type=int,\n",
    "    default=32,\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-output_name',\n",
    "    help='output_name',\n",
    "    type=str,\n",
    "    default='',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-wandb_name',\n",
    "    help='wandb_name',\n",
    "    type=str,\n",
    "    default='siq-finetune',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-val_batch_size',\n",
    "    help='val_batch_size -- defaults to 32',\n",
    "    type=int,\n",
    "    default=32,\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-scan_minibatch',\n",
    "    help='scan_minibatch -- basically, if this is true then batch size is 1 but we do gradient accumulation',\n",
    "    action='store_true',\n",
    "    default=True,\n",
    ")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# print(f\"Loading from {args.config_file}\", flush=True)\n",
    "with open(args.pretrain_config_file, 'r') as f:\n",
    "    config = yaml.load(f, yaml.FullLoader)\n",
    "os.environ[\"TFRECORDS_PATH\"] =  \"/data/raw/siq\"\n",
    "os.environ[\"OUTPUT_PATH\"] = \"/data/raw/models\"\n",
    "config['data']['train_fns'] = os.path.join(os.environ[\"TFRECORDS_PATH\"], \"train{:03d}of833.tfrecord\")\n",
    "config['data']['num_train_files'] = 833\n",
    "config['data']['num_answers'] = 4\n",
    "config['data']['random_scale_max'] = 1.1\n",
    "config['data']['random_scale_min'] = 1.0\n",
    "config['data']['num_segments'] = 7\n",
    "\n",
    "config['device']['batch_size'] = 8\n",
    "config['device']['prefetch_size'] = 0\n",
    "config['device']['n_fns_per_cycle'] = 833\n",
    "\n",
    "NUM_EPOCH = args.ne\n",
    "TRAIN_SIZE = 21 * config['data']['num_train_files']\n",
    "steps_per_epoch = TRAIN_SIZE // config['device']['batch_size']\n",
    "config['optimizer'] = {\n",
    "    'beta_2': 0.98,\n",
    "    'eps': 1e-6,\n",
    "    'learning_rate': args.lr,\n",
    "    'num_train_steps': NUM_EPOCH * steps_per_epoch,\n",
    "    'num_warmup_steps': int(0.5 * steps_per_epoch),\n",
    "    'use_bfloat16_adam': True,\n",
    "    'weight_decay_rate': 0.1,\n",
    "    'do_bias_correction': True,\n",
    "}\n",
    "print(args.ckpt)\n",
    "config['device']['iterations_per_loop'] = steps_per_epoch\n",
    "config['data']['lang_seq_len'] = 256\n",
    "cfg_name = args.pretrain_config_file.split('/')[-1]\n",
    "seattle_time = pytz.utc.localize(datetime.utcnow()).astimezone(pytz.timezone('America/Los_Angeles'))\n",
    "seattle_time = seattle_time.strftime(\"%Y-%m-%d-%H:%M.%S\")\n",
    "\n",
    "config['device']['output_dir'] = os.path.join(os.environ[\"OUTPUT_PATH\"], cfg_name)\n",
    "if args.output_name != '':\n",
    "    config['device']['output_dir'] = os.path.join(config['device']['output_dir'], args.output_name)\n",
    "config['device']['output_dir'] = os.path.join(config['device']['output_dir'], seattle_time)\n",
    "\n",
    "np.random.seed(123456)\n",
    "config['model']['output_grid'] = [args.output_grid_h, args.output_grid_w]\n",
    "ds_train_iter = finetune_input_fn_builder(config, 'tvqa')\n",
    "# _, dummy_batch = next(ds_train_iter)\n",
    "\n",
    "\n",
    "config['_ckpt'] = args.ckpt\n",
    "tags = [cfg_name]\n",
    "if args.output_name != '':\n",
    "    tags.append(args.output_name)\n",
    "# if (jax.process_index() == 0):\n",
    "#     import wandb\n",
    "#wandb.init(config=config, project=args.wandb_name, entity='sherylm', notes=\"Loaded from \"+cfg_name, tags=tags)\n",
    "# else:\n",
    "wandb = None\n",
    "\n",
    "class MerlotReserveTVQA(MerlotReserve):\n",
    "    def setup(self):\n",
    "        super().setup()\n",
    "        self.proj = nn.Dense(features=1, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(stddev=0.02), name='proj',\n",
    "                             use_bias=False)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "\n",
    "        # Encode images (twice)\n",
    "        batch_size, images_per_batch, seq_size, img_dim = batch['images'].shape\n",
    "        imgs_enc = self.vision_encoder(batch['images'].reshape(batch_size * images_per_batch, seq_size, img_dim))['seq_attnpool']\n",
    "        imgs_enc = imgs_enc.reshape(batch_size, images_per_batch, seq_size // 4, self.hidden_size)\n",
    "\n",
    "        # Add the \"first image\"\n",
    "        imgs_enc = jnp.concatenate([\n",
    "            jnp.zeros([batch_size, 1, seq_size // 4, self.hidden_size], dtype=imgs_enc.dtype),\n",
    "            imgs_enc,\n",
    "        ], 1)\n",
    "\n",
    "        # duplicate so that we have one per answer\n",
    "        images_per_batch += 1\n",
    "        batch_size, num_ans_per, joint_seq_len, two_ = batch['textonly_seqs'].shape\n",
    "        imgs_enc = imgs_enc.reshape(batch_size, images_per_batch * seq_size // 4, self.hidden_size).repeat(num_ans_per, axis=0)\n",
    "\n",
    "        #########################\n",
    "        text_toks = batch['textonly_seqs'][..., 0].reshape(batch_size * num_ans_per, joint_seq_len)\n",
    "        textonly_inputs = self.prepare_multimodal_inputs(\n",
    "            tokens=text_toks,\n",
    "            token_segment_idx=batch['textonly_seqs'][..., 1].reshape(batch_size * num_ans_per, joint_seq_len),\n",
    "            vision_input=imgs_enc,\n",
    "        )\n",
    "\n",
    "        # Encode audio\n",
    "        # Audio clips are provided as [batch_size, num_segments, num_audio_subsegments, audio_seq_len, num_mels]\n",
    "        batch_size, num_segments, num_audio_subsegments, audio_seq_len, num_mels = batch['audio_clips'].shape\n",
    "        audio_enc = self.audio_encoder(batch['audio_clips'].reshape(-1, audio_seq_len, num_mels))['seq_attnpool']\n",
    "\n",
    "        _, audio_token_len, hidden_size = audio_enc.shape\n",
    "        num_audio_spans = num_segments * num_audio_subsegments\n",
    "\n",
    "        audio_enc = audio_enc.reshape(batch_size, num_audio_spans, audio_token_len, hidden_size)\n",
    "        audio_enc = audio_enc.repeat(num_ans_per, axis=0)\n",
    "\n",
    "        audio_toks = batch['audio_seqs'][..., 0].reshape(batch_size * num_ans_per, joint_seq_len)\n",
    "        audio_pointers = (jnp.cumsum((audio_toks == AUDIOSPAN).astype(jnp.int32), -1) - 1) // audio_token_len\n",
    "        audio_pointers = audio_pointers % num_audio_spans\n",
    "\n",
    "        audio_inputs = self.prepare_multimodal_inputs(\n",
    "            tokens=batch['audio_seqs'][..., 0].reshape(batch_size * num_ans_per, joint_seq_len),\n",
    "            token_segment_idx=batch['audio_seqs'][..., 1].reshape(batch_size * num_ans_per, joint_seq_len),\n",
    "            vision_input=imgs_enc,\n",
    "            audio_spans=audio_enc,\n",
    "            audio_pointers=audio_pointers,\n",
    "        )\n",
    "        # hack: remove 'first img' from sequence lengths\n",
    "        start_imgs = joint_seq_len + seq_size // 4\n",
    "        for k in ['x', 'rotary_coords', 'attention_mask']:\n",
    "            textonly_inputs[k] = jnp.concatenate([textonly_inputs[k][:, :joint_seq_len],\n",
    "                                                  textonly_inputs[k][:, start_imgs:]], 1)\n",
    "\n",
    "            audio_inputs[k] = jnp.concatenate([audio_inputs[k][:, :joint_seq_len],\n",
    "                                               audio_inputs[k][:, start_imgs:]], 1)\n",
    "\n",
    "        textonly_inputs['attention_mask'] = jnp.concatenate([textonly_inputs['attention_mask'][:, :, :joint_seq_len],\n",
    "                                                             textonly_inputs['attention_mask'][:, :, start_imgs:]], 2)\n",
    "\n",
    "        audio_inputs['attention_mask'] = jnp.concatenate([audio_inputs['attention_mask'][:, :, :joint_seq_len],\n",
    "                                                          audio_inputs['attention_mask'][:, :, start_imgs:]], 2)\n",
    "        #############################################################################################################\n",
    "\n",
    "        # if args.disable_audio:\n",
    "        #     x = textonly_inputs['x']\n",
    "        #     coords = textonly_inputs['rotary_coords']\n",
    "        #     attnmask = textonly_inputs['attention_mask']\n",
    "        #     joint_enc = self.joint_transformer(x, rotary_coords=coords, attention_mask=attnmask)['seq']\n",
    "        #     joint_enc = joint_enc[:, :joint_seq_len].reshape(batch_size * num_ans_per, joint_seq_len, self.hidden_size)\n",
    "        #\n",
    "        #     pool_idx = jnp.argmax((text_toks == MASK).astype(jnp.float32), 1)\n",
    "        #     pooled_h = joint_enc[jnp.arange(batch_size * num_ans_per), pool_idx]\n",
    "        #     logits_from_text = jnp.squeeze(self.proj(pooled_h), -1)\n",
    "        #     logits_from_text = logits_from_text.reshape(batch_size, num_ans_per)\n",
    "        #     logits_from_audio = jnp.ones_like(logits_from_text)\n",
    "        #     return logits_from_audio, logits_from_text\n",
    "\n",
    "\n",
    "        x = jnp.concatenate([audio_inputs['x'], textonly_inputs['x']], 0)\n",
    "        coords = jnp.concatenate([audio_inputs['rotary_coords'], textonly_inputs['rotary_coords']], 0)\n",
    "        attnmask = jnp.concatenate([audio_inputs['attention_mask'], textonly_inputs['attention_mask']], 0)\n",
    "\n",
    "        joint_enc = self.joint_transformer(x, rotary_coords=coords, attention_mask=attnmask)['seq']\n",
    "        joint_enc = joint_enc[:, :joint_seq_len].reshape(batch_size * 2 * num_ans_per, joint_seq_len, self.hidden_size)\n",
    "\n",
    "        # Pool from the right tokens\n",
    "        pool_idx = jnp.argmax((jnp.concatenate([audio_toks, text_toks], 0) == MASK).astype(jnp.float32), 1)\n",
    "        pooled_h = joint_enc[jnp.arange(batch_size * 2 * num_ans_per), pool_idx]\n",
    "        joint_enc = jnp.squeeze(self.proj(pooled_h), -1)\n",
    "\n",
    "        logits_from_audio, logits_from_text = jnp.split(joint_enc, 2, axis=0)\n",
    "        logits_from_audio = logits_from_audio.reshape(batch_size, num_ans_per)\n",
    "        logits_from_text = logits_from_text.reshape(batch_size, num_ans_per)\n",
    "\n",
    "        return logits_from_audio, logits_from_text\n",
    "\n",
    "\n",
    "model = MerlotReserveTVQA.from_config(config)\n",
    "grid_size = (12, 20)\n",
    "#pretrained_model = PretrainedMerlotReserve.from_pretrained(model_name='large', image_grid_size=grid_size)\n",
    "# if args.ckpt == '':\n",
    "#     params = model.init_from_dummy_batch(dummy_batch).unfreeze()\n",
    "# else:\n",
    "params = load_checkpoint(args.ckpt)['params']\n",
    "\n",
    "# Don't need those\n",
    "for k in ['head', 'span_encoder']:\n",
    "    params.pop(k, None)\n",
    "hsz = params['joint_transformer']['final_ln']['bias'].shape[0]\n",
    "params['proj'] = {'kernel': np.random.randn(hsz, 1).astype(np.float32) * 0.01}\n",
    "params = freeze(params)\n",
    "\n",
    "state, tx_fns = construct_finetuning_train_state(opt_config=config['optimizer'], model=model, params=params)\n",
    "\n",
    "def train_loss_fn(state, params, batch):\n",
    "    logits_from_audio, logits_from_text = state.apply_fn({'params': params}, batch)\n",
    "    lprobs_from_audio = jax.nn.log_softmax(logits_from_audio, axis=-1)\n",
    "    lprobs_from_text = jax.nn.log_softmax(logits_from_text, axis=-1)\n",
    "\n",
    "    labels_oh = jax.nn.one_hot(batch['labels'],\n",
    "                               dtype=logits_from_audio.dtype,\n",
    "                               num_classes=logits_from_audio.shape[-1])\n",
    "\n",
    "    loss_audio = -jnp.mean(jnp.sum(labels_oh * lprobs_from_audio, axis=-1))\n",
    "    loss_text = -jnp.mean(jnp.sum(labels_oh * lprobs_from_text, axis=-1))\n",
    "\n",
    "    loss = loss_audio + loss_text\n",
    "    is_right_audio = (jnp.argmax(logits_from_audio, -1) == batch['labels']).astype(jnp.float32).mean()\n",
    "    is_right_text = (jnp.argmax(logits_from_text, -1) == batch['labels']).astype(jnp.float32).mean()\n",
    "\n",
    "    return loss, {'train_audio_acc': is_right_audio, 'train_text_acc': is_right_text,\n",
    "                  'train_audio_loss': loss_audio, 'train_text_loss': loss_text,}\n",
    "\n",
    "\n",
    "p_train_step = jax.pmap(functools.partial(finetune_train_step, loss_fn=train_loss_fn, tx_fns=tx_fns, scan_minibatch=args.scan_minibatch),\n",
    "                                          axis_name='batch', donate_argnums=(0,1))\n",
    "\n",
    "def pred_step(state: train_state.TrainState, batch):\n",
    "    logits_from_audio, logits_from_text = state.apply_fn({'params': state.params}, batch)\n",
    "\n",
    "    out = {'logprobs_audio': jax.nn.log_softmax(logits_from_audio, axis=-1),\n",
    "            'preds_audio': jnp.argmax(logits_from_audio, -1),\n",
    "            'logprobs_text': jax.nn.log_softmax(logits_from_text, axis=-1),\n",
    "            'preds_text': jnp.argmax(logits_from_text, -1),\n",
    "            }\n",
    "    softmax_joint = jax.nn.softmax(logits_from_audio, axis=-1) + jax.nn.softmax(logits_from_text, axis=-1)\n",
    "    out['preds_joint'] = jnp.argmax(softmax_joint, -1)\n",
    "    return out\n",
    "\n",
    "\n",
    "p_pred_step = jax.pmap(pred_step, axis_name='batch', donate_argnums=(1,))\n",
    "\n",
    "\n",
    "def val_epoch(state: train_state.TrainState):\n",
    "    \"\"\"\n",
    "    perform a validation epoch\n",
    "    :param state:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    val_config = deepcopy(config)\n",
    "    val_config['data']['val_fns'] = os.path.join(os.environ[\"TFRECORDS_PATH\"], \"val{:03d}of061.tfrecord\")\n",
    "    val_config['data']['num_val_files'] = 61\n",
    "    val_config['data']['do_random_scale'] = False\n",
    "    val_config['data']['batch_size'] = args.val_batch_size\n",
    "\n",
    "    val_iter = finetune_val_input_fn_builder(val_config, 'tvqa')\n",
    "\n",
    "    text_preds = []\n",
    "    audio_preds = []\n",
    "    joint_preds = []\n",
    "    batch_num = 0\n",
    "    for ids, batch in val_iter:\n",
    "        if batch_num % 50 == 0:\n",
    "            print(\"~~~~VAL_BATCH\", batch_num)\n",
    "        batch_num+=1\n",
    "        val_pred = p_pred_step(state, batch)\n",
    "        preds_joint = val_pred['preds_joint'].reshape(-1)\n",
    "        preds_audio = val_pred['preds_audio'].reshape(-1)\n",
    "        preds_text = val_pred['preds_text'].reshape(-1)\n",
    "\n",
    "        labels = batch['labels'].reshape(-1)\n",
    "        for (p_j, p_a, p_t, id_i, label_i) in zip(val_pred['preds_joint'].reshape(-1),\n",
    "                                                  val_pred['preds_audio'].reshape(-1),\n",
    "                                                  val_pred['preds_text'].reshape(-1), ids, labels):\n",
    "            if id_i == 'pad':\n",
    "                continue\n",
    "            text_preds.append({'pred': p_t, 'label': label_i, 'id': id_i})\n",
    "            audio_preds.append({'pred': p_a, 'label': label_i, 'id': id_i})\n",
    "            joint_preds.append({'pred': p_j, 'label': label_i, 'id': id_i})\n",
    "\n",
    "    text_preds = pd.DataFrame(text_preds)\n",
    "    text_preds['is_right'] = text_preds['pred'] == text_preds['label']\n",
    "    text_acc = text_preds['is_right'].mean()\n",
    "\n",
    "    audio_preds = pd.DataFrame(audio_preds)\n",
    "    audio_preds['is_right'] = audio_preds['pred'] == audio_preds['label']\n",
    "    audio_acc = audio_preds['is_right'].mean()\n",
    "\n",
    "    joint_preds = pd.DataFrame(joint_preds)\n",
    "    joint_preds['is_right'] = joint_preds['pred'] == joint_preds['label']\n",
    "    joint_acc = joint_preds['is_right'].mean()\n",
    "    return {'text_acc': text_acc, 'audio_acc': audio_acc, 'joint_acc': joint_acc}\n",
    "\n",
    "# train_metrics = []\n",
    "# log_every = config['device'].get('commit_every_nsteps', 50)\n",
    "# time_elapsed = []\n",
    "# num_batch = 0\n",
    "\n",
    "# # the + 1 is because for some reason it crashes at the end otherwise. why? idk/\n",
    "# for n in range(config['optimizer']['num_train_steps']+100):\n",
    "#     st = time.time()\n",
    "#     id_, batch = next(ds_train_iter)\n",
    "#     if num_batch % 50 == 0:\n",
    "#         print(\"~~~~~~BATCH\", num_batch)\n",
    "#     num_batch += 1\n",
    "#     state, loss_info = p_train_step(state, batch)\n",
    "\n",
    "#     if jax.process_index() == 0:\n",
    "#         train_metrics.append(jax.tree_map(lambda x: x[0], loss_info))\n",
    "#         jax.tree_map(lambda x: x.copy_to_host_async(), train_metrics[-1])\n",
    "\n",
    "#         step_for_logging = n - log_every\n",
    "#         if step_for_logging >= 0:\n",
    "#             train_metrics[step_for_logging] = {k: float(v) for k, v in train_metrics[step_for_logging].items()}\n",
    "#             if wandb is not None:\n",
    "#                 wandb.log(train_metrics[step_for_logging], step=step_for_logging, commit=True) #(n + 1) % log_every == 0)\n",
    "\n",
    "#         # if (n + 1) % config['device']['iterations_per_loop'] == 0:\n",
    "#         if (n + 1) % 500 == 0: # do val every 500 steps\n",
    "#             print(\"Done 500 steps, doing validation\", flush=True)\n",
    "\n",
    "#             save_checkpoint(state, path=config['device']['output_dir'], no_optimizer=True)\n",
    "#             val_info = val_epoch(state)\n",
    "#             print(f\"Saving @iter {n:03d}.\\nInfo: {pd.Series(val_info)}\\n~\\n\", flush=True)\n",
    "#             if wandb is not None:\n",
    "#                 wandb.log({'joint_acc_val': val_info['joint_acc']}, step=step_for_logging, commit=True)\n",
    "#                 # wandb.log({k + '_val': v for k, v in val_info.items()}, step=step_for_logging, commit=True)\n",
    "#             #shutil.rmtree(\"/data/merlot_reserve/out/base.yaml/\")\n",
    "\n",
    "#         time_elapsed.append(time.time() - st)\n",
    "#         if len(time_elapsed) >= 100:\n",
    "#             tsum = sum(time_elapsed)\n",
    "#             print(\"Completed 100 batches in {:.3f}sec, avg {:.3f} it/sec\".format(tsum, 100.0 / tsum), flush=True)\n",
    "#             time_elapsed = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
